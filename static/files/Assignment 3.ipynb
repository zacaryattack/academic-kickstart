{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    " Zacary Cotton\n",
    " 100 103 1997\n",
    " CSE 5334\n",
    " Assignment 3\n",
    "\n",
    "\n",
    "\n",
    "To successfully execute this .ipynb file, the text dataset dirrectory aclImdb,\n",
    "must be located within the same dirrectory as this .ipynb file.\n",
    "\n",
    "The text dataset dirrectory aclImdb is obtained from\n",
    "url: http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "\n",
    "We begin by importing the necessary libraries and building the necessary paths\n",
    "in order to load the data sets, which upon download are located in a dir called\n",
    "aclImdb, in a train dir and a test dir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T15:47:53.629390Z",
     "start_time": "2020-04-12T15:47:50.962436Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "/Users/zmo/here/jupyter/data mining/ass3/aclImdb/train\n",
      "/Users/zmo/here/jupyter/data mining/ass3/aclImdb/train/pos\n",
      "/Users/zmo/here/jupyter/data mining/ass3/aclImdb/train/neg\n",
      "\n",
      "/Users/zmo/here/jupyter/data mining/ass3/aclImdb/test\n",
      "/Users/zmo/here/jupyter/data mining/ass3/aclImdb/test/pos\n",
      "/Users/zmo/here/jupyter/data mining/ass3/aclImdb/test/neg\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re # for formatting reviews\n",
    "\n",
    "from math import log # for probability representation\n",
    "from sys import path # to locate review txt files\n",
    "from os import listdir # to locate review txt files\n",
    "from os.path import join # to locate review txt files\n",
    "from random import shuffle # to mix up lists\n",
    "from collections import defaultdict # for holding results\n",
    "\n",
    "from sklearn.metrics import accuracy_score # for producing accuracy\n",
    "from sklearn.model_selection import train_test_split # for splitting data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ensure that the .ipynb file is in the\n",
    "# same dir as the unzipped aclImdb dir\n",
    "data_dir_path = path[0] + '/aclImdb'\n",
    "\n",
    "print()\n",
    "\n",
    "train_dir_path = data_dir_path + '/train'\n",
    "print(train_dir_path)\n",
    "train_pos_path = train_dir_path + '/pos'\n",
    "print(train_pos_path)\n",
    "train_neg_path = train_dir_path + '/neg'\n",
    "print(train_neg_path)\n",
    "\n",
    "print()\n",
    "\n",
    "test_dir_path = data_dir_path + '/test'\n",
    "print(test_dir_path)\n",
    "test_pos_path = test_dir_path + '/pos'\n",
    "print(test_pos_path)\n",
    "test_neg_path = test_dir_path + '/neg'\n",
    "print(test_neg_path, end='\\n\\n')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "After having built our paths, next we must read each document 1 by 1 and store\n",
    "each's content in the form of a list, after that we must format each list to\n",
    "rid any elements that are non alphanumeric or tailor excessive whitespaces and\n",
    "set all alpha characters to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T15:49:02.843441Z",
     "start_time": "2020-04-12T15:48:05.448213Z"
    }
   },
   "outputs": [],
   "source": [
    "# returns list of reviews\n",
    "def Load_Reviews(review_dir_path):\n",
    "    \n",
    "    # obatins a list of file paths from the supplied review_dir_path\n",
    "    file_paths = [join(review_dir_path, file) for file in listdir(review_dir_path) if '.txt' in file]\n",
    "    \n",
    "    # each element of reviews is a read txt doc\n",
    "    reviews = []\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, 'r') as file: reviews.append(file.read())\n",
    "\n",
    "    return reviews\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# does (s)light formatting of reviews\n",
    "def Format_Reviews(reviews):\n",
    "    \n",
    "    # removes excessive whitespaces & converts to lowercase\n",
    "    for i, review in enumerate(reviews):\n",
    "        \n",
    "        # only keep alphanumeric characters\n",
    "        review = re.sub('[^0-9a-zA-Z]+', ' ', review.lower())\n",
    "        reviews[i] = ' '.join(review.split())\n",
    "    \n",
    "    return reviews\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# each element of train_pos is a read txt doc\n",
    "train_pos = Load_Reviews(train_pos_path)\n",
    "train_pos = Format_Reviews(train_pos)\n",
    "\n",
    "# each element of train_neg is a read txt doc\n",
    "train_neg = Load_Reviews(train_neg_path)\n",
    "train_neg = Format_Reviews(train_neg)\n",
    "\n",
    "\n",
    "# each element of test_pos is a read txt doc\n",
    "test_pos = Load_Reviews(test_pos_path)\n",
    "test_pos = Format_Reviews(test_pos)\n",
    "\n",
    "# each element of test_neg is a read txt doc\n",
    "test_neg = Load_Reviews(test_neg_path)\n",
    "test_neg = Format_Reviews(test_neg)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "a. Divide the data set as train, development, and test.\n",
    "\n",
    "Now we will start part a. and combine the training and testing data that was\n",
    "supplied in the movie review folders into a single pool of data, shuffle it,\n",
    "and then split this pool into a train, development, and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T15:49:10.750208Z",
     "start_time": "2020-04-12T15:49:10.498839Z"
    }
   },
   "outputs": [],
   "source": [
    "x_training = train_pos + train_neg\n",
    "y_training = [1]*len(train_pos) + [0]*len(train_neg)\n",
    "\n",
    "x_testing = test_pos + test_neg\n",
    "y_testing = [1]*len(test_pos) + [0]*len(test_neg)\n",
    "\n",
    "x_pool = x_training + x_testing\n",
    "y_pool = y_training + y_testing\n",
    "\n",
    "# pools all data into a single source, shuffles it, then\n",
    "# splits into x_train, y_train, x_dev, y_dev, x_test, y_test\n",
    "# x_train, y_train get 70%\n",
    "#     x_dev, y_dev get 15%\n",
    "#   x_test, y_test get 15%\n",
    "pool = list(zip(x_pool, y_pool))\n",
    "shuffle(pool)\n",
    "\n",
    "x_pool, y_pool = zip(*pool)\n",
    "x_all, y_all = x_pool, y_pool\n",
    "\n",
    "##############################################################\n",
    "# a. Divide the data set as train, development, and test.\n",
    "##############################################################\n",
    "x_train, x_dev, y_train, y_dev = train_test_split(x_pool, y_pool, test_size=0.3, random_state=42)\n",
    "x_dev, x_test, y_dev, y_test = train_test_split(x_dev, y_dev, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "b. Build a vocabulary as set not list (for speed increase).\n",
    "\n",
    "Now we define our Naive Bayes Classifier, it has 2 main central functions, Train\n",
    "and Predict. After training the model on the train set we must then demonstrate\n",
    "the accuracy of the model with the development set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T15:49:22.619124Z",
     "start_time": "2020-04-12T15:49:17.681444Z"
    },
    "code_folding": [
     72,
     91
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "c. Calculate the following probability\n",
      "\n",
      "Probability of the occurrence:\n",
      "            P[\"the\"] = ?\n",
      "            P[\"the\"] = 99.543%\n",
      "\n",
      "Conditional Prob based on sentiment:\n",
      " P[\"the\" | Positive] = ?\n",
      " P[\"the\" | Positive] = 99.483%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class NaiveBayes_Classifier:\n",
    "    \n",
    "    def Train(self, x_train, y_train, smoothing_flag):\n",
    "        \n",
    "        # records the known class labels\n",
    "        self.class_labels = set(y_train)\n",
    "        \n",
    "        # separates reviews by classes\n",
    "        self.reviews_by_class = {}\n",
    "        for i, review in enumerate(x_train):\n",
    "            if y_train[i] in self.reviews_by_class: self.reviews_by_class[y_train[i]].append(review)\n",
    "            else: self.reviews_by_class[y_train[i]] = [review]\n",
    "        \n",
    "        # locates length of class with smallest review count\n",
    "        min_len = float('inf')\n",
    "        for class_label in self.class_labels:\n",
    "            shuffle(self.reviews_by_class[class_label])\n",
    "            if len(self.reviews_by_class[class_label]) < min_len:\n",
    "                min_len = len(self.reviews_by_class[class_label])\n",
    "        \n",
    "        # makes all classes have the same review count\n",
    "        for class_label in self.class_labels:\n",
    "            shuffle(self.reviews_by_class[class_label])\n",
    "            self.reviews_by_class[class_label] = self.reviews_by_class[class_label][:min_len]\n",
    "        \n",
    "        \n",
    "        ##############################################################\n",
    "        # b. Build a vocabulary as set not list (for speed increase).\n",
    "        ##############################################################\n",
    "        # records the vocabulary and word stats for words by classes\n",
    "        # builds a vocabulary as set becasue as a list was too slow!!\n",
    "        self.vocab = set(); word_stats = {}\n",
    "        for label in self.class_labels:\n",
    "            word_stats[label] = {}\n",
    "\n",
    "            for review in self.reviews_by_class[label]:\n",
    "                for word in review.split():\n",
    "                    \n",
    "                    self.vocab.add(word)\n",
    "                    if word in word_stats[label]: word_stats[label][word] += 1\n",
    "                    else: word_stats[label][word] = 1\n",
    "        \n",
    "        # build priors & likelihoods\n",
    "        # prior & likelihood but, use log to stop computer\n",
    "        # from rounding to 0 so, log(prior) & log(likelihood)\n",
    "        self.log_priors = {}; self.log_likelihoods = {}\n",
    "        for label in self.class_labels:\n",
    "            # builds a log(prior)\n",
    "            self.log_priors[label] = log(len(self.reviews_by_class[label]) / len(x_train))\n",
    "            \n",
    "            # build a log(likelihood)\n",
    "            self.log_likelihoods[label] = {}\n",
    "            total_count_of_word = sum([word_stats[label][word] for word in self.vocab if word in word_stats[label]])\n",
    "            for word in self.vocab:\n",
    "                numerator = word_stats[label][word] if word in word_stats[label] else 0   \n",
    "                denominator =  total_count_of_word\n",
    "                \n",
    "                # if smoothing, applies Laplace Smoothing\n",
    "                if smoothing_flag == True: numerator += 1; denominator += len(self.vocab)\n",
    "                \n",
    "                if(\n",
    "                    numerator == 0 or\n",
    "                    denominator == 0\n",
    "                ):\n",
    "                    self.log_likelihoods[label][word] = 0\n",
    "                \n",
    "                else: self.log_likelihoods[label][word] = log(numerator/denominator)\n",
    "\n",
    "        return\n",
    "    \n",
    "    \n",
    "    \n",
    "    def Get_WordsProbability(self, word):\n",
    "        \n",
    "        # num of documents containing ‘the’ / num of all documents\n",
    "        \n",
    "        word_in_review_count = 0\n",
    "        total_review_count = 0\n",
    "        \n",
    "        for class_label in self.reviews_by_class:\n",
    "            total_review_count += len(self.reviews_by_class[class_label])            \n",
    "            for review in self.reviews_by_class[class_label]:\n",
    "                if word in review: word_in_review_count += 1\n",
    "                \n",
    "        probability = word_in_review_count / total_review_count\n",
    "        probability *= 100\n",
    "                \n",
    "        return probability\n",
    "    \n",
    "    \n",
    "    \n",
    "    def Get_ConditionalProb_ofWord_based_onSentiment(self, word, sentiment):\n",
    "                \n",
    "        # of positive documents containing “the” / num of all positive review documents\n",
    "        \n",
    "        word_in_pos_review_count = 0\n",
    "        for review in self.reviews_by_class[sentiment]:\n",
    "            if word in review: word_in_pos_review_count += 1\n",
    "            \n",
    "        total_pos_review_count = len(self.reviews_by_class[sentiment])\n",
    "                \n",
    "        probability = word_in_pos_review_count / total_pos_review_count\n",
    "        probability *= 100\n",
    "        \n",
    "        return probability\n",
    "    \n",
    "    \n",
    "    \n",
    "    def Predict(self, reviews):\n",
    "        \n",
    "        # so reviews could be a single string or a list of strings\n",
    "        if isinstance(reviews, list) != True: reviews = [reviews]\n",
    "        \n",
    "        predictions = [0]*len(reviews)\n",
    "        for i, review in enumerate(reviews):\n",
    "            \n",
    "            probs = {}\n",
    "            for label in self.class_labels:\n",
    "                \n",
    "                # for each class label builds the probability,\n",
    "                # incrementally adds log_likelihoods to log_priors\n",
    "                # for each given word in a given review\n",
    "                probs[label] = self.log_priors[label]\n",
    "                for word in review.split():\n",
    "                    if word in self.vocab: probs[label] += self.log_likelihoods[label][word]\n",
    "        \n",
    "            # determines the prediction, picks class with highest probability\n",
    "            if probs[0] < probs[1]: predictions[i] = 1\n",
    "                \n",
    "        return predictions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "nbc = NaiveBayes_Classifier()\n",
    "nbc.Train(x_train, y_train, smoothing_flag=True)\n",
    "\n",
    "##############################################################\n",
    "# c. Calculate the following probability\n",
    "##############################################################\n",
    "print('\\nc. Calculate the following probability')\n",
    "\n",
    "print('\\nProbability of the occurrence:\\n' + ' '*12 + 'P[\\\"the\\\"] = ?')\n",
    "the_prob = nbc.Get_WordsProbability('the')\n",
    "print(' '*12 + 'P[\\\"the\\\"] = {:.3f}%'.format(the_prob))\n",
    "\n",
    "print('\\nConditional Prob based on sentiment:\\n P[\\\"the\\\" | Positive] = ?')\n",
    "the_condprob = nbc.Get_ConditionalProb_ofWord_based_onSentiment('the', 1)\n",
    "print(' P[\\\"the\\\" | Positive] = {:.3f}%\\n'.format(the_condprob))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "d. Calculate accuracy using dev data set\n",
    "\n",
    "Next we will perform part d., calculate accuracy using dev data set and use\n",
    "k-fold cross validation to estimate the skill of the model with k = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T15:50:14.225178Z",
     "start_time": "2020-04-12T15:49:28.949716Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "d. Calculate accuracy using dev data set\n",
      "\n",
      "Calculated accuracy using dev data set: 84.800%\n",
      "\n",
      "Calculated accuracies during conduction of five fold cross validation.\n",
      "   accuracy 1: 84.720%\n",
      "   accuracy 2: 84.720%\n",
      "   accuracy 3: 84.240%\n",
      "   accuracy 4: 83.320%\n",
      "   accuracy 5: 94.150%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class K_Fold_CrossValidation:\n",
    "    \n",
    "    def Prepare_Evaluation(self, k, X, Y):\n",
    "        \n",
    "        self.k = k\n",
    "        \n",
    "        # shuffle the data set randomly\n",
    "        data_set = list(zip(X, Y))        \n",
    "        shuffle(data_set)\n",
    "        \n",
    "        data_set_len = len(data_set)\n",
    "        while data_set_len % self.k != 0: data_set_len -= 1\n",
    "        step = int(data_set_len / self.k)\n",
    "        \n",
    "        # split the data set into k groups\n",
    "        self.groups = []; i = 0; j = step\n",
    "        for k in range(self.k): self.groups.append(data_set[i:j]); i += step; j += step\n",
    "        shuffle(self.groups)\n",
    "                \n",
    "        return\n",
    "    \n",
    "    \n",
    "    \n",
    "    def Evaluate(self):\n",
    "                \n",
    "        scores = []\n",
    "        for i, group in enumerate(self.groups):\n",
    "            # for each group, take the group as a development data set\n",
    "            \n",
    "            group = self.groups.pop(i)\n",
    "            x_dev, y_dev = zip(*group); x_dev = list(x_dev); y_dev = list(y_dev)\n",
    "            \n",
    "            # take the remaining groups as a training data set\n",
    "            train_group = []\n",
    "            for group in self.groups: train_group += group\n",
    "            x_train, y_train = zip(*train_group); x_train = list(x_train); y_train = list(y_train)\n",
    "            \n",
    "            # train a model on the training set & evaluate it on the development set\n",
    "            nbc = NaiveBayes_Classifier()\n",
    "            nbc.Train(x_train, y_train, smoothing_flag=True)\n",
    "\n",
    "            y_pred = nbc.Predict(x_dev)\n",
    "            accuracy = accuracy_score(y_dev, y_pred)*100\n",
    "            \n",
    "            # retain the accuracy score\n",
    "            scores.append(accuracy)\n",
    "            self.groups.insert(i, group)\n",
    "        \n",
    "        print('\\nCalculated accuracies during conduction of five fold cross validation.')\n",
    "        \n",
    "        for i, score in enumerate(scores): print('   accuracy {}: {:.3f}%'.format(i+1, score))\n",
    "        \n",
    "        print()\n",
    "        \n",
    "        return\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "nbc = NaiveBayes_Classifier()\n",
    "nbc.Train(x_train, y_train, smoothing_flag=True)\n",
    "\n",
    "print('\\nd. Calculate accuracy using dev data set')\n",
    "\n",
    "y_pred = nbc.Predict(x_dev)\n",
    "accuracy = accuracy_score(y_dev, y_pred)*100\n",
    "print('\\nCalculated accuracy using dev data set: {:.3f}%'.format(accuracy))\n",
    "\n",
    "kfcv = K_Fold_CrossValidation()\n",
    "kfcv.Prepare_Evaluation(5, x_all, y_all)\n",
    "kfcv.Evaluate()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "e. Do following experiments (1)\n",
    "\n",
    "Experiment 1: Compare the effects of smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T15:50:32.693886Z",
     "start_time": "2020-04-12T15:50:19.426854Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "e. Doing following experiment (1): Compare the effects of Smoothing\n",
      "\n",
      "Naive Bayes Classifier without Smoothing.\n",
      "Calculated accuracy using dev data set: 67.080%\n",
      "\n",
      "Naive Bayes Classifier with Smoothing.\n",
      "Calculated accuracy using dev data set: 84.827%\n",
      "\n",
      "Using smoothing resulted in accuracy that was 17.747% higher.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\ne. Doing following experiment (1): Compare the effects of Smoothing')\n",
    "\n",
    "nbc = NaiveBayes_Classifier()\n",
    "\n",
    "nbc.Train(x_train, y_train, smoothing_flag=False)\n",
    "y_pred = nbc.Predict(x_dev)\n",
    "accuracy_sf = accuracy_score(y_dev, y_pred)*100\n",
    "print('\\nNaive Bayes Classifier without Smoothing.')\n",
    "print('Calculated accuracy using dev data set: {:.3f}%'.format(accuracy_sf))\n",
    "\n",
    "nbc.Train(x_train, y_train, smoothing_flag=True)\n",
    "y_pred = nbc.Predict(x_dev)\n",
    "accuracy_st = accuracy_score(y_dev, y_pred)*100\n",
    "print('\\nNaive Bayes Classifier with Smoothing.')\n",
    "print('Calculated accuracy using dev data set: {:.3f}%'.format(accuracy_st))\n",
    "\n",
    "print('\\nUsing smoothing resulted in accuracy that was {:.3f}% higher.\\n'.format(accuracy_st-accuracy_sf))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "e. Do following experiments (2)\n",
    "\n",
    "Experiment 2: Derive top 10 words that predict positive and negative classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T15:50:38.526011Z",
     "start_time": "2020-04-12T15:50:38.151697Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "e. Doing following experiment (2): Derive Top 10 words that predicts positive and negative classes\n",
      "\n",
      "top 10 words that predict the positive class\n",
      "  word 1: the\n",
      "  word 2: and\n",
      "  word 3: a\n",
      "  word 4: of\n",
      "  word 5: to\n",
      "  word 6: is\n",
      "  word 7: in\n",
      "  word 8: br\n",
      "  word 9: it\n",
      "  word 10: i\n",
      "\n",
      "top 10 words that predict the negative class\n",
      "  word 1: the\n",
      "  word 2: a\n",
      "  word 3: and\n",
      "  word 4: of\n",
      "  word 5: to\n",
      "  word 6: br\n",
      "  word 7: is\n",
      "  word 8: it\n",
      "  word 9: i\n",
      "  word 10: in\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\ne. Doing following experiment (2): Derive Top 10 words that predicts positive and negative classes')\n",
    "\n",
    "results = defaultdict(list)\n",
    "for class_label in nbc.class_labels:\n",
    "    words = list(nbc.log_likelihoods[class_label].keys())\n",
    "    \n",
    "    # creates a list of words and their associated probabilities\n",
    "    results[class_label] = [[word, nbc.log_likelihoods[class_label][word]] for word in words]\n",
    "    \n",
    "    # sorts list in descending order\n",
    "    results[class_label].sort(reverse=True, key=lambda x: x[1])\n",
    "\n",
    "words, probs = zip(*(results[1][:10]))\n",
    "print('\\ntop 10 words that predict the positive class')\n",
    "for i, word in enumerate(words): print('  word ' + str(i+1) + ': ' + word)\n",
    "\n",
    "words, probs = zip(*(results[0][:10]))\n",
    "print('\\ntop 10 words that predict the negative class')\n",
    "for i, word in enumerate(words): print('  word ' + str(i+1) + ': ' + word)\n",
    "print()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "f. Using the test data set\n",
    "\n",
    "Finally we will perform part f. We have seen that the effects of using\n",
    "smoothing are much too impressive to neglect and so we train the model\n",
    "again with the train set and with the optimal hyperparameter being to\n",
    "use smoothing, we then calculate the FINAL accuracy with the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T15:50:49.720740Z",
     "start_time": "2020-04-12T15:50:42.899506Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "f. Using the test data set\n",
      "\n",
      "Naive Bayes Classifier with optimal hyperparameter: Smoothing\n",
      "\n",
      "Calculated FINAL accuracy using test data set: 84.827%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\nf. Using the test data set')\n",
    "\n",
    "nbc = NaiveBayes_Classifier()\n",
    "nbc.Train(x_train, y_train, smoothing_flag=True)\n",
    "\n",
    "y_pred = nbc.Predict(x_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)*100\n",
    "print('\\nNaive Bayes Classifier with optimal hyperparameter: Smoothing')\n",
    "print('\\nCalculated FINAL accuracy using test data set: {:.3f}%\\n'.format(accuracy_st))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
